{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221488c-1f4d-415a-ac01-a0bf9a10cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc36a98-c6e8-4ba4-a55c-8ba58c0d1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-21-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] =\"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11457255-bc8f-4768-acb4-1cd029232dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/14 17:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:01:11 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:01:56 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:02:40 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:03:25 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:04:53 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:05:38 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:06:22 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:07:54 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:08:39 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:09:27 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:10:15 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:11:00 WARN DAGScheduler: Broadcasting large task binary with size 1667.6 KiB\n",
      "25/03/14 17:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:14:00 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:14:43 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:15:27 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:16:13 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:17:02 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:17:48 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:18:34 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:19:19 WARN DAGScheduler: Broadcasting large task binary with size 1667.6 KiB\n",
      "25/03/14 17:20:04 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:20:49 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:21:32 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:22:15 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:22:58 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:23:42 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:24:30 WARN DAGScheduler: Broadcasting large task binary with size 1667.6 KiB\n",
      "25/03/14 17:25:16 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "25/03/14 17:26:00 WARN DAGScheduler: Broadcasting large task binary with size 1667.5 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import urbanity\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lit, expr\n",
    "\n",
    "def inspect_geojson_structure(spark, file_path):\n",
    "    \"\"\"\n",
    "    Inspect the structure of a GeoJSON file to understand its schema.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        file_path: Path to GeoJSON file\n",
    "        \n",
    "    Returns:\n",
    "        Sample row and schema\n",
    "    \"\"\"\n",
    "    # Load the raw data\n",
    "    raw_df = spark.read.format(\"json\").load(file_path)\n",
    "    \n",
    "    # Print the schema\n",
    "    print(\"Raw GeoJSON Schema:\")\n",
    "    raw_df.printSchema()\n",
    "    \n",
    "    # Show a sample row\n",
    "    print(\"Sample row:\")\n",
    "    raw_df.show(1, truncate=False, vertical=True)\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def load_geojson_files_flexible(spark, nodes_path, edges_path, city_name=None):\n",
    "    \"\"\"\n",
    "    Load GeoJSON files with flexible structure handling.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        nodes_path: Path to the nodes GeoJSON file\n",
    "        edges_path: Path to the edges GeoJSON file\n",
    "        city_name: Optional name of the city for identification\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (nodes_df, edges_df)\n",
    "    \"\"\"\n",
    "    # Read raw JSON\n",
    "    nodes_raw = spark.read.format(\"json\").load(nodes_path)\n",
    "    edges_raw = spark.read.format(\"json\").load(edges_path)\n",
    "    \n",
    "    # Print schemas to understand structure\n",
    "    print(\"Nodes Raw Schema:\")\n",
    "    nodes_raw.printSchema()\n",
    "    \n",
    "    # Check if the data follows GeoJSON feature collection pattern\n",
    "    if \"features\" in nodes_raw.columns:\n",
    "        # Standard GeoJSON feature collection\n",
    "        print(\"Processing as GeoJSON feature collection...\")\n",
    "        \n",
    "        # Explode the features array to get individual features\n",
    "        nodes_df = nodes_raw.select(explode(col(\"features\")).alias(\"feature\"))\n",
    "        edges_df = edges_raw.select(explode(col(\"features\")).alias(\"feature\"))\n",
    "        \n",
    "        # Extract properties and geometry from each feature\n",
    "        nodes_df = nodes_df.select(\n",
    "            col(\"feature.properties.*\"),\n",
    "            col(\"feature.geometry.coordinates\").alias(\"coordinates\"),\n",
    "            col(\"feature.geometry.type\").alias(\"geometry_type\")\n",
    "        )\n",
    "        \n",
    "        edges_df = edges_df.select(\n",
    "            col(\"feature.properties.*\"),\n",
    "            col(\"feature.geometry.coordinates\").alias(\"coordinates\"),\n",
    "            col(\"feature.geometry.type\").alias(\"geometry_type\")\n",
    "        )\n",
    "    elif \"properties\" in nodes_raw.columns:\n",
    "        # Each row is a GeoJSON feature\n",
    "        print(\"Processing as individual GeoJSON features...\")\n",
    "        \n",
    "        # Extract properties and geometry directly\n",
    "        nodes_df = nodes_raw.select(\n",
    "            col(\"properties.*\"),\n",
    "            col(\"geometry.coordinates\").alias(\"coordinates\"),\n",
    "            col(\"geometry.type\").alias(\"geometry_type\")\n",
    "        )\n",
    "        \n",
    "        edges_df = edges_raw.select(\n",
    "            col(\"properties.*\"),\n",
    "            col(\"geometry.coordinates\").alias(\"coordinates\"),\n",
    "            col(\"geometry.type\").alias(\"geometry_type\")\n",
    "        )\n",
    "    else:\n",
    "        # Not following standard GeoJSON structure\n",
    "        print(\"Non-standard GeoJSON structure, keeping as-is...\")\n",
    "        nodes_df = nodes_raw\n",
    "        edges_df = edges_raw\n",
    "    \n",
    "    # Add city identifier if provided\n",
    "    if city_name:\n",
    "        nodes_df = nodes_df.withColumn(\"city\", lit(city_name))\n",
    "        edges_df = edges_df.withColumn(\"city\", lit(city_name))\n",
    "    \n",
    "    # Show final schema\n",
    "    print(\"Final nodes schema:\")\n",
    "    # nodes_df.printSchema()\n",
    "    \n",
    "    return nodes_df, edges_df\n",
    "\n",
    "def load_multiple_cities_flexible(spark, city_paths):\n",
    "    \"\"\"\n",
    "    Load data for multiple cities with flexible structure handling.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        city_paths: Dictionary mapping city names to tuples of (nodes_path, edges_path)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (all_nodes_df, all_edges_df)\n",
    "    \"\"\"\n",
    "    all_nodes_dfs = []\n",
    "    all_edges_dfs = []\n",
    "    \n",
    "    for city, (nodes_path, edges_path) in city_paths.items():\n",
    "        print(f\"Loading data for city: {city}\")\n",
    "        nodes_df, edges_df = load_geojson_files_flexible(spark, nodes_path, edges_path, city)\n",
    "        all_nodes_dfs.append(nodes_df)\n",
    "        all_edges_dfs.append(edges_df)\n",
    "    \n",
    "    # Union all city dataframes if we have more than one\n",
    "    if len(all_nodes_dfs) > 1:\n",
    "        # Try unionByName with allowMissingColumns=True for Spark 3.1+\n",
    "        try:\n",
    "            all_nodes_df = all_nodes_dfs[0]\n",
    "            all_edges_df = all_edges_dfs[0]\n",
    "            \n",
    "            for i in range(1, len(all_nodes_dfs)):\n",
    "                all_nodes_df = all_nodes_df.unionByName(all_nodes_dfs[i], allowMissingColumns=True)\n",
    "                all_edges_df = all_edges_df.unionByName(all_edges_dfs[i], allowMissingColumns=True)\n",
    "        except:\n",
    "            # Fallback for older Spark versions - find common columns\n",
    "            print(\"Using fallback union method for older Spark versions\")\n",
    "            node_columns = set(all_nodes_dfs[0].columns)\n",
    "            edge_columns = set(all_edges_dfs[0].columns)\n",
    "            \n",
    "            for df in all_nodes_dfs[1:]:\n",
    "                node_columns = node_columns.intersection(set(df.columns))\n",
    "            \n",
    "            for df in all_edges_dfs[1:]:\n",
    "                edge_columns = edge_columns.intersection(set(df.columns))\n",
    "            \n",
    "            # Select only common columns and union\n",
    "            all_nodes_df = all_nodes_dfs[0].select(*node_columns)\n",
    "            all_edges_df = all_edges_dfs[0].select(*edge_columns)\n",
    "            \n",
    "            for i in range(1, len(all_nodes_dfs)):\n",
    "                all_nodes_df = all_nodes_df.union(all_nodes_dfs[i].select(*node_columns))\n",
    "                all_edges_df = all_edges_df.union(all_edges_dfs[i].select(*edge_columns))\n",
    "    else:\n",
    "        all_nodes_df = all_nodes_dfs[0]\n",
    "        all_edges_df = all_edges_dfs[0]\n",
    "    \n",
    "    return all_nodes_df, all_edges_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"GeoJSON data loader\") \\\n",
    "        .config(\"spark.driver.memory\", \"10g\") \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Example: Inspect a single file first to understand structure\n",
    "    sample_file = \"data/Adelaide_nodes_100m.geojson\"\n",
    "    inspect_geojson_structure(spark, sample_file)\n",
    "    print('-------')\n",
    "    sample_file = \"data/Adelaide_edges_100m.geojson\"\n",
    "    inspect_geojson_structure(spark, sample_file)\n",
    "    print('-------')\n",
    "    \n",
    "    # Dictionary mapping city names to file paths\n",
    "    city_paths = {\n",
    "        # \"new_york\": (\"data/new_york_nodes.geojson\", \"data/new_york_edges.geojson\"),\n",
    "        # \"san_francisco\": (\"data/san_francisco_nodes.geojson\", \"data/san_francisco_edges.geojson\")\n",
    "    }\n",
    "    for i in os.listdir(\"data/\"):\n",
    "        if \"nodes\" in i.lower():\n",
    "            city_paths[i.split(\"_\")[0]] = [\"data/\"+i, None]\n",
    "    for i in os.listdir(\"data/\"):\n",
    "        if \"edges\" in i.lower():\n",
    "            city_paths[i.split(\"_\")[0]][1] = \"data/\"+i\n",
    "    \n",
    "    # Load all cities with the flexible loader\n",
    "    nodes_df, edges_df = load_multiple_cities_flexible(spark, city_paths)\n",
    "    \n",
    "    # Cache dataframes for faster processing\n",
    "    nodes_df.cache()\n",
    "    edges_df.cache()\n",
    "\n",
    "    nodes_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .partitionBy(\"city\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"./street_networks/nodes\")\n",
    "    \n",
    "    edges_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .partitionBy(\"city\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"./street_networks/edges\")\n",
    "    \n",
    "    # Alternatively, repartition before saving\n",
    "    nodes_df.repartition(10).write.format(\"parquet\").mode(\"overwrite\").save(\"./street_networks/nodes\")\n",
    "    edges_df.repartition(10).write.format(\"parquet\").mode(\"overwrite\").save(\"./street_networks/edges\")\n",
    "    os.makedirs(\"street_networks/nodes/\", exist_ok=True)\n",
    "    # If DataFrames are very large, save in smaller chunks\n",
    "    city_list = [row.city for row in nodes_df.select(\"city\").distinct().collect()]\n",
    "    for city in city_list:\n",
    "        city_nodes = nodes_df.filter(f\"city = '{city}'\")\n",
    "        city_edges = edges_df.filter(f\"city = '{city}'\")\n",
    "        \n",
    "        city_nodes.write.format(\"parquet\").mode(\"overwrite\").save(f\"./street_networks/nodes/{city}\")\n",
    "        city_edges.write.format(\"parquet\").mode(\"overwrite\").save(f\"./street_networks/edges/{city}\")\n",
    "    # # Save for future use\n",
    "    # nodes_df.write.format(\"parquet\").mode(\"overwrite\").save(\"street_networks/nodes\")\n",
    "    # edges_df.write.format(\"parquet\").mode(\"overwrite\").save(\"street_networks/edges\")\n",
    "    \n",
    "    # # Show schema and sample data\n",
    "    # print(\"Nodes schema:\")\n",
    "    # nodes_df.printSchema()\n",
    "    \n",
    "    # print(\"Edges schema:\")\n",
    "    # edges_df.printSchema()\n",
    "    \n",
    "    # print(\"Sample nodes data:\")\n",
    "    # nodes_df.show(5, truncate=False)\n",
    "    \n",
    "    # print(\"Sample edges data:\")\n",
    "    # edges_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8818bb5-50a5-486b-92eb-f311a8416b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o1579.write",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnodes_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreet_networks_3/nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m edges_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreet_networks_3/edges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyspark/sql/dataframe.py:509\u001b[0m, in \u001b[0;36mDataFrame.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameWriter:\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tab2\")\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyspark/sql/readwriter.py:964\u001b[0m, in \u001b[0;36mDataFrameWriter.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o1579.write"
     ]
    }
   ],
   "source": [
    "nodes_df.write.format(\"parquet\").mode(\"overwrite\").save(\"street_networks_3/nodes\")\n",
    "edges_df.write.format(\"parquet\").mode(\"overwrite\").save(\"street_networks_3/edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa499c8-a425-4688-8383-ccce9b67f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample nodes data:\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(\"Nodes schema:\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# nodes_df.printSchema()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"Edges schema:\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# edges_df.printSchema()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample nodes data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnodes_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample edges data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m edges_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyspark/sql/dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         },\n\u001b[1;32m    976\u001b[0m     )\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# print(\"Nodes schema:\")\n",
    "# nodes_df.printSchema()\n",
    "\n",
    "# print(\"Edges schema:\")\n",
    "# edges_df.printSchema()\n",
    "\n",
    "print(\"Sample nodes data:\")\n",
    "nodes_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Sample edges data:\")\n",
    "edges_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1eb6cf-cb0a-4d9d-9b60-5bc7cf8b1eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanity",
   "language": "python",
   "name": "urbanity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
